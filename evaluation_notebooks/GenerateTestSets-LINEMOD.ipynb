{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=3\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=3\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import quat_math\n",
    "import pickle\n",
    "import glob\n",
    "\n",
    "from PIL import Image\n",
    "import scipy.io as scio\n",
    "from functools import partial\n",
    "from object_pose_utils.utils import to_np, to_var\n",
    "from object_pose_utils.utils.display import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = 20, 12\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 unused import\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Object Indices of Interest\n",
    "\n",
    "| Object Indices |[]()|[]()|\n",
    "|---|---|---|\n",
    "| __1.__ 002_master_chef_can | __8.__ 009_gelatin_box      | __15.__ 035_power_drill       |\n",
    "| __2.__ 003_cracker_box     | __9.__ 010_potted_meat_can  | __16.__ 036_wood_block        |\n",
    "| __3.__ 004_sugar_box       | __10.__ 011_banana          | __17.__ 037_scissors          |\n",
    "| __4.__ 005_tomato_soup_can | __11.__ 019_pitcher_base    | __18.__ 040_large_marker      |\n",
    "| __5.__ 006_mustard_bottle  | __12.__ 021_bleach_cleanser | __19.__ 051_large_clamp       |\n",
    "| __6.__ 007_tuna_fish_can   | __13.__ 024_bowl            | __20.__ 052_extra_large_clamp |\n",
    "| __7.__ 008_pudding_box     | __14.__ 025_mug             | __21.__ 061_foam_brick        |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transforms3d.quaternions import quat2mat, mat2quat\n",
    "\n",
    "def getPoseCNNQuat(data, obj):\n",
    "    pose_idx = np.where(data['rois'][:,1].flatten()==obj)[0]\n",
    "    if(len(pose_idx) == 0):\n",
    "        return None\n",
    "    else:\n",
    "        pose_idx = pose_idx[0]\n",
    "    pose = data['poses'][pose_idx]\n",
    "    q = pose[:4][[1,2,3,0]]\n",
    "    q /= np.linalg.norm(q)\n",
    "    t = pose[4:7]\n",
    "    return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LINEMOD Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_pose_utils.datasets.linemod_dataset import LinemodDataset\n",
    "from object_pose_utils.datasets.image_processing import ImageNormalizer\n",
    "#from object_pose_utils.datasets.inplane_rotation_augmentation import InplaneRotator\n",
    "\n",
    "from object_pose_utils.datasets.pose_dataset import OutputTypes as otypes\n",
    "\n",
    "dataset_root = \"/scratch/datasets/linemod/Linemod_preprocessed/\"\n",
    "object_list = [1,2,4,5,6,8,9,10,11,12,13,14,15]\n",
    "mode = \"eval\"\n",
    "num_objects = 13 #number of object classes in the dataset\n",
    "num_points = 500 #number of points on the input pointcloud\n",
    "\n",
    "num_objects = 13 #number of object classes in the dataset\n",
    "num_points = 500 #number of points on the input pointcloud\n",
    "\n",
    "output_format = [otypes.OBJECT_LABEL,\n",
    "                 otypes.QUATERNION, \n",
    "                 otypes.TRANSLATION, \n",
    "                 otypes.IMAGE_CROPPED,\n",
    "                 otypes.DEPTH_POINTS_MASKED_AND_INDEXES,\n",
    "                ]\n",
    "\n",
    "dataset = LinemodDataset(dataset_root, \n",
    "                         mode = mode,\n",
    "                         object_list = object_list,#[object_id], \n",
    "                         output_data = output_format,\n",
    "                         add_syn_noise = False,\n",
    "                         add_syn_background = False,\n",
    "                         resample_on_error = False,\n",
    "                         #preprocessors = [InplaneRotator(0)],\n",
    "                         postprocessors = [ImageNormalizer()],\n",
    "                         image_size = [640, 480], \n",
    "                         num_points=500,\n",
    "                         segnet_mask = True)\n"
    "dataset = LinemodDataset(dataset_root, mode=mode,                          \n",
    "                         segnet_masks = True,\n",
    "                         object_list = object_list,\n",
    "                         output_data = output_format,\n",
    "                         resample_on_error = False,\n",
    "                         add_syn_background = False,\n",
    "                         add_syn_noise = False,\n",
    "                         use_posecnn_data = True,\n",
    "                         #preprocessors = [InplaneRotator()],\n",
    "                         postprocessors = [ImageNormalizer()],\n",
    "                         image_size = [640, 480], num_points=500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pose Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weights = '/home/bokorn/src/DenseFusion/trained_checkpoints/linemod/pose_model_9_0.01310166542980859.pth'\n",
    "df_refine_weights = '/home/bokorn/src/DenseFusion/trained_checkpoints/linemod/pose_refine_model_493_0.006761023565178073.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65a0dacd04314f788b2fa3548c4e67e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13407), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception on index 584: No points in mask\n",
      "Exception on index 7699: No points in mask\n",
      "Exception on index 7700: No points in mask\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from dense_fusion.network import PoseNet, PoseRefineNet\n",
    "from generic_pose.utils.evaluation_utils import fullEvaluateDenseFusion\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pickle\n",
    "\n",
    "if(df_refine_weights is not None):\n",
    "    df_refine_estimator = PoseRefineNet(num_points = num_points, num_obj = num_objects)\n",
    "    df_refine_estimator.load_state_dict(torch.load(df_refine_weights))\n",
    "    df_refine_estimator.cuda();\n",
    "    df_refine_estimator.eval();\n",
    "else:\n",
    "    df_refine_estimator = None\n",
    "\n",
    "df_estimator = PoseNet(num_points = num_points, num_obj = num_objects)\n",
    "\n",
    "df_estimator.load_state_dict(torch.load(df_weights))\n",
    "df_estimator.cuda();\n",
    "df_estimator.eval();\n",
    "\n",
    "results = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for j, data in enumerate(tqdm(dataset)):\n",
    "        obj, quat, trans, img, points, choose = data\n",
    "\n",
    "        if(len(obj) == 0):\n",
    "            results[j] = None\n",
    "            continue\n",
    "\n",
    "        #obj = torch.nonzero(torch.LongTensor(dataset.object_labels) == obj)[0] + 1\n",
    "        \n",
    "        res = fullEvaluateDenseFusion(df_estimator, img, points, choose, obj+1)\n",
    "        max_q, max_t, max_c, max_feat, pred_q, pred_t, pred_c, refine_q, refine_t, global_feat = res\n",
    "        \n",
    "        results[j] = {'max_q':max_q,\n",
    "                      'max_t':max_t,\n",
    "                      'max_c':max_c,\n",
    "                      'max_feat':max_feat,\n",
    "                      'pred_q':pred_q,\n",
    "                      'pred_t':pred_t,\n",
    "                      'pred_c':pred_c,\n",
    "                      'refine_q':refine_q,\n",
    "                      'refine_t':refine_t,\n",
    "                      'global_feat':global_feat}\n",
    "\n",
    "results_filename = 'results_linemod_test_df_' + '.'.join(df_weights.split('/')[-1].split('.')[:-1])\n",
    "if(df_refine_weights is not None):\n",
    "    results_filename += '_' + '.'.join(df_refine_weights.split('/')[-1].split('.')[:-1])\n",
    "results_filename += '.pkl'\n",
    "\n",
    "with open(results_filename, 'wb') as f:\n",
    "    pickle.dump(results, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'results_linemod_test_df_pose_model_9_0.01310166542980859_pose_refine_model_493_0.006761023565178073.pkl'"
      ]
     },
     "execution_count": 7,
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_filename"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (bpy)",
   "language": "python",
   "name": "bpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
