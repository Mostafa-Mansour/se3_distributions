{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=1\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import quat_math\n",
    "import pickle\n",
    "import glob\n",
    "\n",
    "from PIL import Image\n",
    "import scipy.io as scio\n",
    "from functools import partial\n",
    "from object_pose_utils.utils import to_np, to_var\n",
    "from object_pose_utils.utils.display import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = 20, 12\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 unused import\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Object Indices of Interest\n",
    "\n",
    "| Object Indices |[]()|[]()|\n",
    "|---|---|---|\n",
    "| __1.__ 002_master_chef_can | __8.__ 009_gelatin_box      | __15.__ 035_power_drill       |\n",
    "| __2.__ 003_cracker_box     | __9.__ 010_potted_meat_can  | __16.__ 036_wood_block        |\n",
    "| __3.__ 004_sugar_box       | __10.__ 011_banana          | __17.__ 037_scissors          |\n",
    "| __4.__ 005_tomato_soup_can | __11.__ 019_pitcher_base    | __18.__ 040_large_marker      |\n",
    "| __5.__ 006_mustard_bottle  | __12.__ 021_bleach_cleanser | __19.__ 051_large_clamp       |\n",
    "| __6.__ 007_tuna_fish_can   | __13.__ 024_bowl            | __20.__ 052_extra_large_clamp |\n",
    "| __7.__ 008_pudding_box     | __14.__ 025_mug             | __21.__ 061_foam_brick        |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_pose_utils.utils.pose_error import accuracyAUC\n",
    "dataset_root = '/ssd0/datasets/ycb/YCB_Video_Dataset'\n",
    "\n",
    "non_sym_objs = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 17, 18,]\n",
    "sym_objs = [13, 16, 19, 20, 21]\n",
    "textureless_objects = set([10, 11, 13, 14, 16, 19, 20, 21])\n",
    "textured_objects = set(range(1,22)) - set([10, 11, 13, 14, 16, 19, 20, 21])\n",
    "\n",
    "\n",
    "with open('{0}/image_sets/classes.txt'.format(dataset_root)) as f:                                    \n",
    "    classes = f.read().split()\n",
    "classes.insert(0, '__background__')\n",
    "\n",
    "def non_func(x):\n",
    "    return x\n",
    "\n",
    "def mean_abs(x):\n",
    "    return np.mean(np.abs(x))\n",
    "\n",
    "def log_clean(x):\n",
    "    return np.log(np.maximum(1e-6, x))\n",
    "\n",
    "def mean_log_clean(x):\n",
    "    return np.mean(log_clean(x))\n",
    "\n",
    "def mean_log(x):\n",
    "    return np.mean(np.log(x))   \n",
    "\n",
    "def accuracyAUC100(x, max_theshold):\n",
    "    return accuracyAUC(x, max_theshold)*100\n",
    "\n",
    "def median_log(x):\n",
    "    return np.median(np.log(x))\n",
    "\n",
    "def makeTableEntries(data_dict, keys = None, objs = list(range(1,22)), \n",
    "              val_func = non_func, bold_func = max,\n",
    "              individual = False,\n",
    "              agg_title = 'All'):\n",
    "    if(keys is None):\n",
    "        keys = list(data_dict.keys())\n",
    "    \n",
    "    data_str = ''\n",
    "    if(individual):\n",
    "        for obj in objs:    \n",
    "            data_str += ' '.join(classes[obj].split('_')[1:]) + '\\n'\n",
    "            vals = [val_func(np.array(data_dict[k][obj])) for k in keys]\n",
    "            bold_val = bold_func(vals)\n",
    "            for v in vals:\n",
    "                if(v == bold_val):\n",
    "                    data_str += ' & \\\\textbf{' + '{:0.2f}'.format(v) + '}'\n",
    "                else:\n",
    "                    data_str += ' & {:0.2f}'.format(v)\n",
    "            data_str += ' \\\\\\\\'\n",
    "            data_str += '\\n'\n",
    "\n",
    "        data_str += '\\hline\\n'\n",
    "\n",
    "    data_str += '{} \\n'.format(agg_title)\n",
    "    vals = [val_func(np.concatenate([np.array(data_dict[k][o]) for o in objs])) for k in keys]\n",
    "    bold_val = bold_func(vals)\n",
    "    for v in vals:\n",
    "        if(v == bold_val):\n",
    "            data_str += ' & \\\\textbf{' + '{:0.2f}'.format(v) + '}'\n",
    "        else:\n",
    "            data_str += ' & {:0.2f}'.format(v)\n",
    "    data_str += ' \\\\\\\\'\n",
    "    data_str += '\\n'\n",
    "    #print(data_str)\n",
    "    return data_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transforms3d.quaternions import quat2mat, mat2quat\n",
    "\n",
    "def getPoseCNNQuat(data, obj):\n",
    "    pose_idx = np.where(data['rois'][:,1].flatten()==obj)[0]\n",
    "    if(len(pose_idx) == 0):\n",
    "        return None\n",
    "    else:\n",
    "        pose_idx = pose_idx[0]\n",
    "    pose = data['poses'][pose_idx]\n",
    "    q = pose[:4][[1,2,3,0]]\n",
    "    q /= np.linalg.norm(q)\n",
    "    t = pose[4:7]\n",
    "    return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YCB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_pose_utils.datasets.linemod_dataset import LinemodDataset\n",
    "from object_pose_utils.datasets.image_processing import ImageNormalizer\n",
    "from object_pose_utils.datasets.sixdc_dataset import ply_vtx\n",
    "\n",
    "from object_pose_utils.datasets.pose_dataset import OutputTypes as otypes\n",
    "\n",
    "\n",
    "dataset_root = \"/ssd0/datasets/linemod/Linemod_preprocessed/\"\n",
    "object_list = [1,2,4,5,6,8,9,10,11,12,13,14,15]\n",
    "\n",
    "#object_id = 1\n",
    "\n",
    "output_format = [otypes.OBJECT_LABEL,\n",
    "                 otypes.QUATERNION, \n",
    "                 otypes.TRANSLATION, \n",
    "                 otypes.IMAGE_CROPPED,\n",
    "                 otypes.DEPTH_POINTS_MASKED_AND_INDEXES,\n",
    "                ]\n",
    "\n",
    "dataset = LinemodDataset(dataset_root, \n",
    "                         mode = 'eval',\n",
    "                         object_list = object_list,#[object_id], \n",
    "                         output_data = output_format,\n",
    "                         add_syn_noise = False,\n",
    "                         add_syn_background = False,\n",
    "                         resample_on_error = False,\n",
    "                         #preprocessors = [InplaneRotator(0)],\n",
    "                         postprocessors = [ImageNormalizer()],\n",
    "                         image_size = [640, 480], \n",
    "                         num_points=500,\n",
    "                         segnet_mask = True)\n",
    "\n",
    "\n",
    "model_clouds = {}\n",
    "for j, object_id in enumerate(object_list):\n",
    "    model_filename = '{}/models/obj_{:02d}.ply'.format(dataset_root, object_id)\n",
    "\n",
    "    model_cloud = ply_vtx(model_filename)/1000.\n",
    "    model_cloud = model_cloud[np.random.choice(len(model_cloud), 1000, replace=False)]\n",
    "    \n",
    "    model_clouds[j] = model_cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pose Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dense_fusion.network import PoseNet\n",
    "\n",
    "model_checkpoint = '/home/bokorn/src/DenseFusion/trained_checkpoints/linemod/pose_model_9_0.01310166542980859.pth'\n",
    "\n",
    "num_objects = 13 #number of object classes in the dataset\n",
    "num_points = 500 #number of points on the input pointcloud\n",
    "df_estimator = PoseNet(num_points = num_points, \n",
    "                    num_obj = num_objects)\n",
    "df_estimator.load_state_dict(torch.load(model_checkpoint))\n",
    "df_estimator.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_filename = 'results_linemod_test_df_pose_model_9_0.01310166542980859_pose_refine_model_493_0.006761023565178073.pkl'\n",
    "\n",
    "with open(df_results_filename, 'rb') as f:\n",
    "    df_results = pickle.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution Estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mlr_1e-5\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls /scratch/bokorn/results/log_lik_linemod/df_global_comp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from se3_distributions.losses.loglik_loss import evaluateFeature\n",
    "from se3_distributions.models.compare_networks import SigmoidCompareNet, SigmoidNet\n",
    "\n",
    "#feature_root = '/scratch/bokorn/results/dense_fusion_global_feat/'\n",
    "feature_root = '/scratch/datasets/linemod/'\n",
    "feature_key = 'feat_global'\n",
    "feature_size = 1024\n",
    "\n",
    "grid_vertices = torch.load(os.path.join(feature_root, 'grid',\n",
    "    '{}_vertices.pt'.format(dataset.classes[1])))\n",
    "\n",
    "grid_features = {}\n",
    "for j, object_id in enumerate(object_list):\n",
    "    grid_features[j+1] = torch.load(os.path.join(feature_root, 'grid',\n",
    "        '{}_{}_features.pt'.format(feature_key, dataset.classes[object_id])))\n",
    "\n",
    "comp_model_checkpoint = '/scratch/bokorn/results/log_lik_linemod/df_global_comp/lr_1e-5/2020-02-28_20-38-07/weights/checkpoint_91000.pth'\n",
    "\n",
    "comp_estimator = SigmoidCompareNet(feature_size, len(object_list))\n",
    "comp_estimator.load_state_dict(torch.load(comp_model_checkpoint))\n",
    "comp_estimator.cuda();\n",
    "comp_estimator.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_root = '/scratch/datasets/ycb/'\n",
    "feature_key = 'feat_global'\n",
    "feature_size = 1024\n",
    "grid_size = 3885\n",
    "\n",
    "reg_model_checkpoint = '/scratch/bokorn/results/log_lik_linemod/df_global_reg/lr_1e-5/2020-02-28_21-13-23/weights/checkpoint_197000.pth'\n",
    "\n",
    "reg_estimator = SigmoidNet(feature_size, len(object_list)*grid_size)\n",
    "reg_estimator.load_state_dict(torch.load(reg_model_checkpoint))\n",
    "reg_estimator.cuda();\n",
    "reg_estimator.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(False):\n",
    "    confusion_root = '/home/bokorn/data/confusion_matrices'\n",
    "    confusion_eps = 0.000001\n",
    "\n",
    "    confusion_matrices = {}\n",
    "    for object_id in object_list:\n",
    "        confusion_matrices[object_id] = scio.loadmat(os.path.join(confusion_root, \n",
    "            \"{0}_confusion_matrix.mat\".format(object_id)))['loaded'] + confusion_eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_pose_utils.utils.bingham import iso_loss_calculation, duel_loss_calculation\n",
    "\n",
    "class isoLikelihood(object):\n",
    "        def __init__(self, mean_q, sig):\n",
    "            self.mean_q = mean_q\n",
    "            self.sig = sig.flatten()\n",
    "            \n",
    "        def __call__(self, quats):\n",
    "            likelihoods = []\n",
    "            for q in quats.unsqueeze(1):\n",
    "                loss, lik = iso_loss_calculation(self.mean_q, torch.abs(self.sig), q)\n",
    "                likelihoods.append(lik*2.0)\n",
    "            return torch.stack(likelihoods)\n",
    "        \n",
    "class duelLikelihood(object):\n",
    "        def __init__(self, mean_q, duel_q, z):\n",
    "            self.mean_q = mean_q\n",
    "            self.duel_q = duel_q\n",
    "            self.z = z.flatten()\n",
    "            \n",
    "        def __call__(self, quats):\n",
    "            likelihoods = []\n",
    "            for q in quats.unsqueeze(1):\n",
    "                loss, lik = duel_loss_calculation(self.mean_q, self.duel_q, \n",
    "                                                  -torch.abs(self.z), q)\n",
    "                likelihoods.append(lik*2.0)\n",
    "            return torch.stack(likelihoods)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/scratch/bokorn/results/log_lik_linemod/df_global_iso/lr_1e-5/2020-02-28_21-36-08/weights/checkpoint_1751000.pth', '/scratch/bokorn/results/log_lik_linemod/df_global_iso/lr_1e-5/2020-02-28_21-36-08/weights/best_quat.pth']\n"
     ]
    }
   ],
   "source": [
    "from se3_distributions.models.bingham_networks import IsoBingham\n",
    "from se3_distributions.losses.bingham_loss import isoLikelihood\n",
    "\n",
    "if(True):\n",
    "    feature_size = 1024\n",
    "    #feature_size = 1408\n",
    "    #iso_model_checkpoint_pattern = '/scratch/bokorn/results/log_lik/df_local_full_orig_iso/**/weights/checkpoint_*.pth'\n",
    "    #iso_model_checkpoint = sorted(glob.glob(iso_model_checkpoint_pattern,\n",
    "    #                                        recursive=True))[-1]\n",
    "    iso_model_checkpoint = '/scratch/bokorn/results/log_lik_linemod/df_global_iso/lr_1e-5/2020-02-28_21-36-08/weights/best_quat.pth'\n",
    "    print(glob.glob('/'.join(iso_model_checkpoint.split('/')[:-1]) + '/*', recursive=True))\n",
    "    iso_estimator = IsoBingham(feature_size, len(object_list))\n",
    "    iso_estimator.load_state_dict(torch.load(iso_model_checkpoint))\n",
    "    iso_estimator.eval()\n",
    "    iso_estimator.cuda()\n",
    "\n",
    "    def bing_iso(res, obj):\n",
    "        max_q, max_t, feat = res['max_q'], res['max_t'], res['global_feat']\n",
    "        \n",
    "        #feat = torch.Tensor(feat).unsqueeze(0).cuda()\n",
    "        mean_est = torch.Tensor(max_q).unsqueeze(0).cuda()\n",
    "        df_obj = torch.LongTensor(obj-1).unsqueeze(0).cuda()\n",
    "        sig_est = iso_estimator(feat.unsqueeze(0).cuda(), df_obj)\n",
    "        lik_est = isoLikelihood(mean_q=mean_est[0], \n",
    "                                sig=sig_est[0,0])\n",
    "        \n",
    "        return lik_est, max_q, max_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_pose_utils.utils.bingham import isobingham_likelihood\n",
    "\n",
    "sigmas_data = np.load('/home/bokorn/src/DenseFusion/df_orig_sigmas.npz', allow_pickle=True)\n",
    "sigmas_single = sigmas_data['single_sigma'].item()\n",
    "sigmas_mixture = sigmas_data['mixture_sigma'].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_pose_utils.utils.pose_processing import quatAngularDiffBatch\n",
    "from object_pose_utils.utils.interpolation import BinghamInterpolation, TetraInterpolation\n",
    "from se3_distributions.utils.evaluation_utils import evaluateDenseFusion\n",
    "\n",
    "def hist_reg_global(img, points, choose, obj):\n",
    "    max_q, max_t, feat = evaluateDenseFusion(df_estimator, img, points, choose, obj)\n",
    "    lik_est = evaluateFeature(reg_estimator, obj, feat, None)\n",
    "    lik_est = to_np(lik_est.flatten())\n",
    "    lik_est /= lik_est.sum()\n",
    "    return lik_est, max_q, max_t\n",
    "\n",
    "def hist_reg_idv_global(img, points, choose, obj):\n",
    "    max_q, max_t, feat = evaluateDenseFusion(df_estimator, img, points, choose, obj)\n",
    "    lik_est = evaluateFeature(reg_estimator_idv[obj.item()], obj, feat, None)\n",
    "    lik_est = to_np(lik_est.flatten())\n",
    "    lik_est /= lik_est.sum()\n",
    "    return lik_est, max_q, max_t\n",
    "\n",
    "def hist_comp_global(img, points, choose, obj):\n",
    "    max_q, max_t, feat = evaluateDenseFusion(df_estimator, img, points, choose, obj)\n",
    "    lik_est = evaluateFeature(comp_estimator, obj, feat, grid_features)\n",
    "    lik_est = to_np(lik_est.flatten())\n",
    "    lik_est /= lik_est.sum()\n",
    "    return lik_est, max_q, max_t\n",
    "\n",
    "def hist_comp_local(img, points, choose, obj):\n",
    "    max_q, max_t, feat = evaluateDenseFusion(df_estimator, img, points, choose, obj, use_global_feat=False)\n",
    "    lik_est = evaluateFeature(comp_estimator_local, obj, feat, grid_features_local)\n",
    "    lik_est = to_np(lik_est.flatten())\n",
    "    lik_est /= lik_est.sum()\n",
    "    return lik_est, max_q, max_t\n",
    "\n",
    "def hist_comp_global_prop(img, points, choose, obj):\n",
    "    max_q, max_t, feat = evaluateDenseFusion(df_estimator, img, points, choose, obj)\n",
    "    lik_est = evaluateFeature(prop_comp_estimator, obj, feat, grid_features)\n",
    "    lik_est = to_np(lik_est.flatten())\n",
    "    lik_est /= lik_est.sum()\n",
    "    return lik_est, max_q, max_t\n",
    "\n",
    "def hist_comp_global_funnel(img, points, choose, obj):\n",
    "    max_q, max_t, feat = evaluateDenseFusion(df_estimator, img, points, choose, obj)\n",
    "    lik_est = evaluateFeature(funnel_comp_estimator, obj, feat, grid_features)\n",
    "    lik_est = to_np(lik_est.flatten())\n",
    "    lik_est /= lik_est.sum()\n",
    "    return lik_est, max_q, max_t\n",
    "\n",
    "def hist_conf(img, points, choose, obj):\n",
    "    max_q, max_t, feat = evaluateDenseFusion(df_estimator, img, points, choose, obj)\n",
    "    dists = quatAngularDiffBatch(max_q, to_np(grid_vertices))\n",
    "    bin_idx = np.argmin(dists)\n",
    "    lik_est = confusion_matrices[obj.item()][bin_idx]\n",
    "    lik_est /= lik_est.sum()\n",
    "    return lik_est, max_q, max_t\n",
    "\n",
    "def bing_fixed(img, points, choose, obj):\n",
    "    max_q, max_t, feat = evaluateDenseFusion(df_estimator, img, points, choose, obj)\n",
    "    bingham = BinghamInterpolation(torch.Tensor(max_q).unsqueeze(0).cuda(), \n",
    "                                   torch.ones(1).cuda(), \n",
    "                                   sigma=torch.Tensor(sigmas_single[obj.item()]).cuda())\n",
    "    return bingham, max_q, max_t\n",
    "    \n",
    "def bing_mixed(img, points, choose, obj):\n",
    "    pred_q, pred_t, pred_c, _ = evaluateDenseFusion(df_estimator, img, points, choose, obj,\n",
    "                                                    return_all = True)\n",
    "    how_max, which_max = torch.max(pred_c, 1)\n",
    "    max_q = to_np(pred_q[which_max.item()])\n",
    "    max_t = to_np(pred_t[which_max.item()])\n",
    "    bingham_mixture = BinghamInterpolation(pred_q.cuda(), pred_c.flatten().cuda(), \n",
    "                                           sigma=torch.Tensor(sigmas_mixture[obj.item()]).cuda())\n",
    "    \n",
    "    return bingham_mixture, max_q, max_t\n",
    "\n",
    "def bing_droput(img, points, choose, obj):\n",
    "    pred_q, pred_t, pred_c, _ = evaluateDenseFusion(df_dropout_estimator, img, points, choose, obj,\n",
    "                                                    return_all = True)\n",
    "    how_max, which_max = torch.max(pred_c, 1)\n",
    "    max_q = to_np(pred_q[which_max.item()])\n",
    "    max_t = to_np(pred_t[which_max.item()])\n",
    "    bingham_mixture = BinghamInterpolation(pred_q.cuda(), pred_c.flatten().cuda(), \n",
    "                                           sigma=torch.Tensor(sigmas_mixture[obj.item()]).cuda())\n",
    "    \n",
    "    return bingham_mixture, max_q, max_t\n",
    "\n",
    "lik_funcs = {'hist_reg_global':hist_reg_global,\n",
    "             #'hist_reg_idv_global':hist_reg_idv_global,\n",
    "             'hist_comp_global':hist_comp_global,\n",
    "             #'hist_comp_global_prop':hist_comp_global_prop,\n",
    "             #'hist_comp_global_funnel':hist_comp_global_funnel,\n",
    "             #'hist_comp_local':hist_comp_local,\n",
    "             #'hist_conf':hist_conf,\n",
    "             #'bing_fixed':bing_fixed,\n",
    "             #'bing_mixed':bing_mixed,\n",
    "             #'bing_iso':bing_iso,\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([6.0243390e-38, 9.8212585e-18, 6.5214782e-21, ..., 6.7959160e-15,\n",
       "        3.8903968e-13, 4.3493822e-13], dtype=float32),\n",
       " array([-0.6172123 , -0.72789514,  0.24433826, -0.171804  ], dtype=float32),\n",
       " array([ 0.05819113, -0.07106589,  0.9622421 ], dtype=float32))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func(img, points, choose, obj+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c858cdbf724b470bab46d95a74600a6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13407), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception on index 584: No points in mask\n",
      "Exception on index 7699: No points in mask\n",
      "Exception on index 7700: No points in mask\n"
     ]
    }
   ],
   "source": [
    "from se3_distributions.datasets.ycb_dataset import getYCBSymmeties\n",
    "from object_pose_utils.utils.pose_processing import symmetricAngularDistance, meanShift\n",
    "from object_pose_utils.utils.pose_error import add, adi\n",
    "from quat_math import quaternion_matrix\n",
    "\n",
    "tetra_interp = TetraInterpolation(2)\n",
    "\n",
    "import pathlib\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "likelihood = {k:{obj:[] for obj in range(len(object_list))} for k in lik_funcs.keys()}\n",
    "\n",
    "sym_angular_error = {k:{obj:[] for obj in range(len(object_list))} for k in lik_funcs.keys()}\n",
    "add_error = {k:{obj:[] for obj in range(len(object_list))} for k in lik_funcs.keys()}\n",
    "add_sym_error = {k:{obj:[] for obj in range(len(object_list))} for k in lik_funcs.keys()}\n",
    "\n",
    "sym_angular_error_mode = {k:{obj:[] for obj in range(len(object_list))} for k in lik_funcs.keys()}\n",
    "add_error_mode = {k:{obj:[] for obj in range(len(object_list))} for k in lik_funcs.keys()}\n",
    "add_sym_error_mode = {k:{obj:[] for obj in range(len(object_list))} for k in lik_funcs.keys()}\n",
    "\n",
    "bad_data = []\n",
    "with torch.no_grad():\n",
    "    for j, data in enumerate(tqdm(dataset)):\n",
    "        obj, quat, trans, img, points, choose = data\n",
    "\n",
    "        if(len(obj) == 0):\n",
    "            bad_data.append(j)\n",
    "            continue\n",
    "\n",
    "        #sym_axis, sym_ang = getYCBSymmeties(obj.item())\n",
    "        sym_axis, sym_ang = [],[]\n",
    "        trans = to_np(trans)\n",
    "\n",
    "        for k, func in lik_funcs.items():            \n",
    "            lik_est, q_est, t_est = func(img, points, choose, obj+1)\n",
    "\n",
    "            if(type(lik_est) in [BinghamInterpolation, isoLikelihood, duelLikelihood]):\n",
    "                lik = lik_est(quat.unsqueeze(0).cuda()).item()\n",
    "                q_mode = q_est\n",
    "            else:\n",
    "                if(type(lik_est) is torch.Tensor):\n",
    "                    lik_est = to_np(lik_est.flatten())\n",
    "                tetra_interp.setValues(lik_est)\n",
    "                lik = tetra_interp.smooth(to_np(quat)).item()    \n",
    "                q_mode = grid_vertices[np.argmax(lik_est)]\n",
    "                #v_shift = meanShift(q_mode.cuda(), grid_vertices.cuda(), dist_est.cuda(),\n",
    "                #                    sigma=np.pi/9, max_iter = 100)\n",
    "\n",
    "            mat = quaternion_matrix(quat)\n",
    "\n",
    "            err_ang = symmetricAngularDistance(torch.Tensor(q_est).unsqueeze(0), \n",
    "                                               quat.unsqueeze(0),\n",
    "                                               sym_axis, sym_ang).item()*180/np.pi\n",
    "            mat_est = quaternion_matrix(q_est)\n",
    "            err_add = add(mat[:3,:3], trans, mat_est[:3,:3], t_est, \n",
    "                          model_clouds[obj.item()])\n",
    "            err_adi = adi(mat[:3,:3], trans, mat_est[:3,:3], t_est, \n",
    "                          model_clouds[obj.item()])\n",
    "\n",
    "            err_ang_mode = symmetricAngularDistance(torch.Tensor(q_mode).unsqueeze(0), \n",
    "                                                    quat.unsqueeze(0),\n",
    "                                                    sym_axis, sym_ang).item()*180/np.pi\n",
    "\n",
    "            mat_mode = quaternion_matrix(q_mode)\n",
    "            err_add_mode = add(mat[:3,:3], trans, mat_mode[:3,:3], t_est, \n",
    "                               model_clouds[obj.item()])\n",
    "            err_adi_mode = adi(mat[:3,:3], trans, mat_mode[:3,:3], t_est, \n",
    "                               model_clouds[obj.item()])\n",
    "\n",
    "            likelihood[k][obj.item()].append(lik)            \n",
    "            sym_angular_error[k][obj.item()].append(err_ang)\n",
    "            add_error[k][obj.item()].append(err_add)\n",
    "            add_sym_error[k][obj.item()].append(err_adi)\n",
    "\n",
    "            sym_angular_error_mode[k][obj.item()].append(err_ang_mode)\n",
    "            add_error_mode[k][obj.item()].append(err_add_mode)\n",
    "            add_sym_error_mode[k][obj.item()].append(err_adi_mode)\n",
    "\n",
    "    np.savez('iros_results/linemod_{}.npz'.format('_'.join(lik_funcs.keys())), \n",
    "             likelihood=likelihood,\n",
    "             sym_angular_error=sym_angular_error,\n",
    "             add_error=add_error,\n",
    "             add_sym_error=add_sym_error,\n",
    "             sym_angular_error_mode=sym_angular_error_mode,\n",
    "             add_error_mode=add_error_mode,\n",
    "             add_sym_error_mode=add_sym_error_mode,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([1, 2, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_features.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(makeTableEntries(likelihood, val_func=mean_log_clean, bold_func = max, individual = True))\n",
    "#makeTable(sym_angular_error, val_func=mean_abs, bold_func = min, individual = True)\n",
    "#print('\\n\\n\\n')\n",
    "#makeTable(add_error, val_func=mean_abs, bold_func = min, individual = True)\n",
    "#print('\\n\\n\\n')\n",
    "#makeTable(add_sym_error, val_func=mean_abs, bold_func = min, individual = True)\n",
    "#print('\\n\\n\\n')\n",
    "#makeTableEntries(sym_angular_error_mode, val_func=mean_abs, bold_func = min, individual = True)\n",
    "#print('\\n\\n\\n')\n",
    "#makeTableEntries(add_error_mode, val_func=mean_abs, bold_func = min, individual = True)\n",
    "#print('\\n\\n\\n')\n",
    "#makeTableEntries(add_sym_error_mode, val_func=mean_abs, bold_func = min, individual = True)\n",
    "#print('\\n\\n\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(makeTableEntries(likelihood, objs = non_sym_objs, val_func=mean_log_clean, bold_func = max, agg_title = 'Non-Symmetric'))\n",
    "print(makeTableEntries(likelihood, objs = sym_objs, val_func=mean_log_clean, bold_func = max, agg_title = 'Symmetric'))\n",
    "print(makeTableEntries(likelihood, val_func=mean_log_clean, bold_func = max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (bpy)",
   "language": "python",
   "name": "bpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
