{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=3\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=3\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import quat_math\n",
    "import pickle\n",
    "import glob\n",
    "\n",
    "from PIL import Image\n",
    "import scipy.io as scio\n",
    "from functools import partial\n",
    "from object_pose_utils.utils import to_np, to_var\n",
    "from object_pose_utils.utils.display import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = 20, 12\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 unused import\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Object Indices of Interest\n",
    "\n",
    "| Object Indices |[]()|[]()|\n",
    "|---|---|---|\n",
    "| __1.__ 002_master_chef_can | __8.__ 009_gelatin_box      | __15.__ 035_power_drill       |\n",
    "| __2.__ 003_cracker_box     | __9.__ 010_potted_meat_can  | __16.__ 036_wood_block        |\n",
    "| __3.__ 004_sugar_box       | __10.__ 011_banana          | __17.__ 037_scissors          |\n",
    "| __4.__ 005_tomato_soup_can | __11.__ 019_pitcher_base    | __18.__ 040_large_marker      |\n",
    "| __5.__ 006_mustard_bottle  | __12.__ 021_bleach_cleanser | __19.__ 051_large_clamp       |\n",
    "| __6.__ 007_tuna_fish_can   | __13.__ 024_bowl            | __20.__ 052_extra_large_clamp |\n",
    "| __7.__ 008_pudding_box     | __14.__ 025_mug             | __21.__ 061_foam_brick        |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transforms3d.quaternions import quat2mat, mat2quat\n",
    "\n",
    "def getPoseCNNQuat(data, obj):\n",
    "    pose_idx = np.where(data['rois'][:,1].flatten()==obj)[0]\n",
    "    if(len(pose_idx) == 0):\n",
    "        return None\n",
    "    else:\n",
    "        pose_idx = pose_idx[0]\n",
    "    pose = data['poses'][pose_idx]\n",
    "    q = pose[:4][[1,2,3,0]]\n",
    "    q /= np.linalg.norm(q)\n",
    "    t = pose[4:7]\n",
    "    return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YCB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_pose_utils.datasets.ycb_dataset import YcbDataset as YCBDataset\n",
    "from object_pose_utils.datasets.image_processing import ImageNormalizer\n",
    "from object_pose_utils.datasets.inplane_rotation_augmentation import InplaneRotator\n",
    "\n",
    "from object_pose_utils.datasets.pose_dataset import OutputTypes as otypes\n",
    "\n",
    "dataset_root = '/ssd0/datasets/ycb/YCB_Video_Dataset'\n",
    "object_list = list(range(1,22))\n",
    "mode = \"valid\"\n",
    "\n",
    "output_format = [otypes.OBJECT_LABEL,\n",
    "                 otypes.QUATERNION, \n",
    "                 otypes.TRANSLATION, \n",
    "                 otypes.IMAGE_CROPPED,\n",
    "                 otypes.DEPTH_POINTS_MASKED_AND_INDEXES,\n",
    "                ]\n",
    "\n",
    "dataset = YCBDataset(dataset_root, mode=mode,\n",
    "                     object_list = list(range(1,22)),\n",
    "                     output_data = output_format,\n",
    "                     resample_on_error = False,\n",
    "                     add_syn_background = False,\n",
    "                     add_syn_noise = False,\n",
    "                     use_posecnn_data = True,\n",
    "                     #preprocessors = [InplaneRotator()],\n",
    "                     postprocessors = [ImageNormalizer()],\n",
    "                     image_size = [640, 480], num_points=1000)\n",
    "\n",
    "model_clouds = {}\n",
    "for object_id in object_list:\n",
    "    cloud_filename = '{}/models/{}/points.xyz'.format(dataset_root, dataset.classes[object_id])\n",
    "    model_clouds[object_id] = np.loadtxt(cloud_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pose Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weights = '/home/bokorn/src/DenseFusion/trained_models/ycb_global/pose_model_13_0.02780649198161978.pth'\n",
    "#df_weights = '/home/bokorn/src/DenseFusion/trained_checkpoints/ycb/pose_model_26_0.012863246640872631.pth'\n",
    "df_refine_weights = '/home/bokorn/src/DenseFusion/trained_checkpoints/ycb/pose_refine_model_69_0.009449292959118935.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa46eba3f40642ae88b14bc91d2483d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1250), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception on index 40807: Mask 0091/000241 has less than minimum number of pixels (2 < 50)\n"
     ]
    }
   ],
   "source": [
    "from dense_fusion.network import PoseNet, PoseNetGlobal, PoseRefineNet\n",
    "from generic_pose.utils.evaluation_utils import fullEvaluateDenseFusion\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pickle\n",
    "\n",
    "if(df_refine_weights is not None):\n",
    "    df_refine_estimator = PoseRefineNet(num_points = 1000, num_obj = 21)\n",
    "    df_refine_estimator.load_state_dict(torch.load(df_refine_weights))\n",
    "    df_refine_estimator.cuda();\n",
    "    df_refine_estimator.eval();\n",
    "else:\n",
    "    df_refine_estimator = None\n",
    "\n",
    "if(df_weights.split('/')[-2] == 'ycb_global'):\n",
    "    df_estimator = PoseNetGlobal(num_points = 1000, num_obj = 21)\n",
    "else:\n",
    "    df_estimator = PoseNet(num_points = 1000, num_obj = 21)\n",
    "\n",
    "df_estimator.load_state_dict(torch.load(df_weights))\n",
    "df_estimator.cuda();\n",
    "df_estimator.eval();\n",
    "\n",
    "results = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for j, data in enumerate(tqdm(dataset)):\n",
    "        obj, quat, trans, img, points, choose = data\n",
    "\n",
    "        if(len(obj) == 0):\n",
    "            results[j] = None\n",
    "            continue\n",
    "\n",
    "        res = fullEvaluateDenseFusion(df_estimator, img, points, choose, obj)\n",
    "        max_q, max_t, max_c, max_feat, pred_q, pred_t, pred_c, refine_q, refine_t, global_feat = res\n",
    "        \n",
    "        results[j] = {'max_q':max_q,\n",
    "                      'max_t':max_t,\n",
    "                      'max_c':max_c,\n",
    "                      'max_feat':max_feat,\n",
    "                      'pred_q':pred_q,\n",
    "                      'pred_t':pred_t,\n",
    "                      'pred_c':pred_c,\n",
    "                      'refine_q':refine_q,\n",
    "                      'refine_t':refine_t,\n",
    "                      'global_feat':global_feat}\n",
    "\n",
    "results_filename = 'results_'+mode+'_df_' + '.'.join(df_weights.split('/')[-1].split('.')[:-1])\n",
    "if(df_refine_weights is not None):\n",
    "    results_filename += '_' + '.'.join(df_refine_weights.split('/')[-1].split('.')[:-1])\n",
    "results_filename += '.pkl'\n",
    "\n",
    "with open(results_filename, 'wb') as f:\n",
    "    pickle.dump(results, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'results_valid_df_pose_model_26_0.012863246640872631_pose_refine_model_69_0.009449292959118935.pkl'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_filename\n",
    "#len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41456"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
=======
>>>>>>> 14f93ff66f94fc50bb338b837ffd3320074446fa
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_weights.split('/')[-2] == 'ycb_global'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (bpy)",
   "language": "python",
   "name": "bpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
