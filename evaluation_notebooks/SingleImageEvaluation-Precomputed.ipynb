{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=3\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=3\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import quat_math\n",
    "import pickle\n",
    "import glob\n",
    "\n",
    "from PIL import Image\n",
    "import scipy.io as scio\n",
    "from functools import partial\n",
    "from object_pose_utils.utils import to_np, to_var\n",
    "from object_pose_utils.utils.display import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = 20, 12\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 unused import\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Object Indices of Interest\n",
    "\n",
    "| Object Indices |[]()|[]()|\n",
    "|---|---|---|\n",
    "| __1.__ 002_master_chef_can | __8.__ 009_gelatin_box      | __15.__ 035_power_drill       |\n",
    "| __2.__ 003_cracker_box     | __9.__ 010_potted_meat_can  | __16.__ 036_wood_block        |\n",
    "| __3.__ 004_sugar_box       | __10.__ 011_banana          | __17.__ 037_scissors          |\n",
    "| __4.__ 005_tomato_soup_can | __11.__ 019_pitcher_base    | __18.__ 040_large_marker      |\n",
    "| __5.__ 006_mustard_bottle  | __12.__ 021_bleach_cleanser | __19.__ 051_large_clamp       |\n",
    "| __6.__ 007_tuna_fish_can   | __13.__ 024_bowl            | __20.__ 052_extra_large_clamp |\n",
    "| __7.__ 008_pudding_box     | __14.__ 025_mug             | __21.__ 061_foam_brick        |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transforms3d.quaternions import quat2mat, mat2quat\n",
    "\n",
    "def getPoseCNNQuat(data, obj):\n",
    "    pose_idx = np.where(data['rois'][:,1].flatten()==obj)[0]\n",
    "    if(len(pose_idx) == 0):\n",
    "        return None\n",
    "    else:\n",
    "        pose_idx = pose_idx[0]\n",
    "    pose = data['poses'][pose_idx]\n",
    "    q = pose[:4][[1,2,3,0]]\n",
    "    q /= np.linalg.norm(q)\n",
    "    t = pose[4:7]\n",
    "    return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YCB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_pose_utils.datasets.ycb_dataset import YcbDataset as YCBDataset\n",
    "from object_pose_utils.datasets.image_processing import ImageNormalizer\n",
    "from object_pose_utils.datasets.inplane_rotation_augmentation import InplaneRotator\n",
    "\n",
    "from object_pose_utils.datasets.pose_dataset import OutputTypes as otypes\n",
    "\n",
    "dataset_root = '/ssd0/datasets/ycb/YCB_Video_Dataset'\n",
    "object_list = list(range(1,22))\n",
    "mode = \"test\"\n",
    "\n",
    "output_format = [otypes.OBJECT_LABEL,\n",
    "                 otypes.QUATERNION, \n",
    "                 otypes.TRANSLATION, \n",
    "                 otypes.TRANSFORM_MATRIX,\n",
    "                ]\n",
    "\n",
    "dataset = YCBDataset(dataset_root, mode=mode,\n",
    "                     object_list = list(range(1,22)),\n",
    "                     output_data = output_format,\n",
    "                     resample_on_error = False,\n",
    "                     add_syn_background = False,\n",
    "                     add_syn_noise = False,\n",
    "                     #use_posecnn_data = True,\n",
    "                     #preprocessors = [InplaneRotator()],\n",
    "                     postprocessors = [ImageNormalizer()],\n",
    "                     image_size = [640, 480], num_points=1000)\n",
    "\n",
    "model_clouds = {}\n",
    "for object_id in object_list:\n",
    "    cloud_filename = '{}/models/{}/points.xyz'.format(dataset_root, dataset.classes[object_id])\n",
    "    model_clouds[object_id] = np.loadtxt(cloud_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pose Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weights = '/home/bokorn/src/DenseFusion/trained_checkpoints/ycb/pose_model_26_0.012863246640872631.pth'\n",
    "df_refine_weights = '/home/bokorn/src/DenseFusion/trained_checkpoints/ycb/pose_refine_model_69_0.009449292959118935.pth'\n",
    "df_global_weights = '/home/bokorn/src/DenseFusion/trained_models/ycb_global/pose_model_13_0.02780649198161978.pth'\n",
    "\n",
    "df_results_filename = 'results_test_df_' + '.'.join(df_weights.split('/')[-1].split('.')[:-1])\n",
    "if(df_refine_weights is not None):\n",
    "    df_results_filename += '_' + '.'.join(df_refine_weights.split('/')[-1].split('.')[:-1])\n",
    "df_results_filename += '.pkl'\n",
    "\n",
    "with open(df_results_filename, 'rb') as f:\n",
    "    df_results = pickle.load(f)\n",
    "    \n",
    "#df_global_results_filename = 'results_test_df_' + '.'.join(df_weights.split('/')[-1].split('.')[:-1])\n",
    "#if(df_refine_weights is not None):\n",
    "#    df_global_results_filename += '_' + '.'.join(df_refine_weights.split('/')[-1].split('.')[:-1])\n",
    "#df_global_results_filename += '.pkl'\n",
    "\n",
    "#with open(df_global_results_filename, 'rb') as f:\n",
    "#    df_global_results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution Estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from se3_distributions.losses.loglik_loss import evaluateFeature\n",
    "from se3_distributions.models.compare_networks import SigmoidCompareNet\n",
    "\n",
    "if(False):\n",
    "    feature_root = '/scratch/bokorn/results/dense_fusion_local_orig_feat/'\n",
    "    feature_key = 'feat_global'\n",
    "    feature_size = 1024\n",
    "    #feature_root = '/scratch/bokorn/results/dense_fusion_local_orig_feat/'\n",
    "    #feature_key = 'feat'\n",
    "    #feature_size = 1408\n",
    "\n",
    "    grid_vertices = torch.load(os.path.join(feature_root, 'grid',\n",
    "        '{}_vertices.pt'.format(dataset.classes[1])))\n",
    "\n",
    "    grid_features = {}\n",
    "    for object_id in object_list:\n",
    "        grid_features[object_id] = torch.load(os.path.join(feature_root, 'grid',\n",
    "            '{}_{}_features.pt'.format(feature_key, dataset.classes[object_id])))\n",
    "\n",
    "    #comp_model_checkpoint_pattern = '/scratch/bokorn/results/log_lik/df_global_orig_comp/**/checkpoint_*.pth'\n",
    "    #comp_model_checkpoint = sorted(glob.glob(comp_model_checkpoint_pattern,\n",
    "    #                                        recursive=True))[-1]\n",
    "    comp_model_checkpoint = '/scratch/bokorn/results/log_lik/df_global_orig_comp/lr_1e-5/2019-09-03_22-37-08/weights/best_quat.pth'\n",
    "    print(glob.glob('/'.join(comp_model_checkpoint.split('/')[:-1]) + '/*', recursive=True))\n",
    "\n",
    "    comp_estimator = SigmoidCompareNet(feature_size, len(object_list))\n",
    "    comp_estimator.load_state_dict(torch.load(comp_model_checkpoint))\n",
    "    comp_estimator.cuda();\n",
    "    comp_estimator.eval();\n",
    "    def hist_comp_global(res, obj):\n",
    "        max_q, max_t, feat = res['max_q'], res['max_t'], res['global_feat']\n",
    "        lik_est = evaluateFeature(comp_estimator, obj, feat, grid_features)\n",
    "        lik_est = to_np(lik_est.flatten())\n",
    "        lik_est /= lik_est.sum()\n",
    "        return lik_est, max_q, max_t\n",
    "    \n",
    "if(False):\n",
    "    feature_root = '/scratch/bokorn/results/dense_fusion_local_orig_feat/'\n",
    "    feature_key = 'feat'\n",
    "    feature_size = 1408\n",
    "\n",
    "    grid_vertices = torch.load(os.path.join(feature_root, 'grid',\n",
    "        '{}_vertices.pt'.format(dataset.classes[1])))\n",
    "\n",
    "    local_grid_features = {}\n",
    "    for object_id in object_list:\n",
    "        local_grid_features[object_id] = torch.load(os.path.join(feature_root, 'grid',\n",
    "            '{}_{}_features.pt'.format(feature_key, dataset.classes[object_id])))\n",
    "\n",
    "    #local_comp_model_checkpoint_pattern = '/scratch/bokorn/results/log_lik/df_local_orig_comp/**/checkpoint_*.pth'\n",
    "    #local_comp_model_checkpoint = sorted(glob.glob(local_comp_model_checkpoint_pattern,\n",
    "    #                                        recursive=True))[-1]\n",
    "    local_comp_model_checkpoint = '/scratch/bokorn/results/log_lik/df_local_orig_comp/lr_1e-5/2019-09-10_06-34-39/weights/best_quat.pth'\n",
    "    print(glob.glob('/'.join(local_comp_model_checkpoint.split('/')[:-1]) + '/*', recursive=True))\n",
    "\n",
    "    local_comp_estimator = SigmoidCompareNet(feature_size, len(object_list))\n",
    "    local_comp_estimator.load_state_dict(torch.load(local_comp_model_checkpoint))\n",
    "    local_comp_estimator.cuda();\n",
    "    local_comp_estimator.eval();\n",
    "    def hist_comp_local(res, obj):\n",
    "        max_q, max_t, feat = res['max_q'], res['max_t'], res['max_feat']\n",
    "        feat = torch.Tensor(feat).unsqueeze(0)\n",
    "        lik_est = evaluateFeature(local_comp_estimator, obj, feat, local_grid_features)\n",
    "        lik_est = to_np(lik_est.flatten())\n",
    "        lik_est /= lik_est.sum()\n",
    "        return lik_est, max_q, max_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from se3_distributions.models.compare_networks import SigmoidNet\n",
    "\n",
    "if(False):\n",
    "    feature_size = 1024\n",
    "    grid_size = 3885\n",
    "\n",
    "    #reg_model_checkpoint_pattern = '/scratch/bokorn/results/log_lik/df_global_orig_reg/lr_1e-5/**/weights/checkpoint_*.pth'\n",
    "    #\n",
    "    #reg_model_checkpoint = sorted(glob.glob(reg_model_checkpoint_pattern,\n",
    "    #                                        recursive=True))[-1]\n",
    "    reg_model_checkpoint = '/scratch/bokorn/results/log_lik/df_global_orig_reg/lr_1e-5/2019-09-04_09-51-12/weights/best_quat.pth'\n",
    "    print(glob.glob('/'.join(reg_model_checkpoint.split('/')[:-1]) + '/*', recursive=True))\n",
    "\n",
    "    reg_estimator = SigmoidNet(feature_size, len(object_list)*grid_size)\n",
    "    reg_estimator.load_state_dict(torch.load(reg_model_checkpoint))\n",
    "    reg_estimator.cuda();\n",
    "    reg_estimator.eval();\n",
    "\n",
    "    def hist_reg_global(res, obj):\n",
    "        max_q, max_t, feat = res['max_q'], res['max_t'], res['global_feat']\n",
    "        lik_est = evaluateFeature(reg_estimator, obj, feat, None)\n",
    "        lik_est = to_np(lik_est.flatten())\n",
    "        lik_est /= lik_est.sum()\n",
    "        return lik_est, max_q, max_t\n",
    "    \n",
    "if(False):\n",
    "    feature_size = 1408\n",
    "    grid_size = 3885\n",
    "\n",
    "    #local_reg_model_checkpoint_pattern = '/scratch/bokorn/results/log_lik/df_local_orig_reg/lr_1e-5/2019-09-10_05-05-04/weights/checkpoint_13000.pth'\n",
    "    #local_reg_model_checkpoint = sorted(glob.glob(local_reg_model_checkpoint_pattern,\n",
    "    #                                        recursive=True))[-1]\n",
    "    local_reg_model_checkpoint = '/scratch/bokorn/results/log_lik/df_local_orig_reg/lr_1e-5/2019-09-10_05-05-04/weights/best_quat.pth'\n",
    "    print(glob.glob('/'.join(local_reg_model_checkpoint.split('/')[:-1]) + '/*', recursive=True))\n",
    "\n",
    "    local_reg_estimator = SigmoidNet(feature_size, len(object_list)*grid_size)\n",
    "    local_reg_estimator.load_state_dict(torch.load(local_reg_model_checkpoint))\n",
    "    local_reg_estimator.cuda();\n",
    "    local_reg_estimator.eval();\n",
    "\n",
    "    def hist_reg_local(res, obj):\n",
    "        max_q, max_t, feat = res['max_q'], res['max_t'], res['max_feat']\n",
    "        feat = torch.Tensor(feat).unsqueeze(0)\n",
    "        lik_est = evaluateFeature(local_reg_estimator, obj, feat, None)\n",
    "        lik_est = to_np(lik_est.flatten())\n",
    "        lik_est /= lik_est.sum()\n",
    "        return lik_est, max_q, max_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from se3_distributions.models.compare_networks import SigmoidNet\n",
    "\n",
    "if(False):\n",
    "    feature_root = '/scratch/bokorn/results/dense_fusion_global_feat/'\n",
    "    feature_key = 'feat_global'\n",
    "    feature_size = 1024\n",
    "    grid_size = 3885\n",
    "    reg_model_idv_checkpoint_pattern = '/scratch/bokorn/results/log_lik/df_global_reg_orig_all/{}/lr_1e-5/**/weights/final_*.pth'\n",
    "\n",
    "    reg_estimator_idv = {}\n",
    "    for object_id in object_list:\n",
    "        reg_model_idv_checkpoint = sorted(glob.glob(reg_model_idv_checkpoint_pattern.format(object_id), recursive=True))[-1]\n",
    "        reg_estimator_idv[object_id] = SigmoidNet(feature_size, grid_size)\n",
    "        reg_estimator_idv[object_id].load_state_dict(torch.load(reg_model_idv_checkpoint))\n",
    "        reg_estimator_idv[object_id].cuda();\n",
    "        reg_estimator_idv[object_id].eval();\n",
    "\n",
    "\n",
    "    def hist_reg_indv_global(res, obj):\n",
    "        max_q, max_t, feat = res['max_q'], res['max_t'], res['global_feat']\n",
    "        lik_est = evaluateFeature(reg_estimator_idv[obj.item()], obj, feat, None)\n",
    "        lik_est = to_np(lik_est.flatten())\n",
    "        lik_est /= lik_est.sum()\n",
    "        return lik_est, max_q, max_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_pose_utils.utils.pose_processing import quatAngularDiffBatch\n",
    "\n",
    "if(False):\n",
    "    #confusion_root = '/home/bokorn/data/confusion_matrices/'\n",
    "    confusion_root = '/home/bokorn/src/se3_distributions/se3_distributions/evaluation_notebooks/conf_matices/densefusion_orig/'\n",
    "    confusion_eps = 0.000001\n",
    "\n",
    "    confusion_matrices = {}\n",
    "    for object_id in object_list:\n",
    "        #confusion_matrices[object_id] = scio.loadmat(os.path.join(confusion_root, \n",
    "        #    \"{0}_confusion_matrix.mat\".format(object_id)))['loaded'] + confusion_eps\n",
    "        confusion_matrices[object_id] = np.load(os.path.join(confusion_root, \n",
    "            \"{0}_confusion_matrix.npy\".format(object_id))) + confusion_eps\n",
    "\n",
    "    def hist_conf(res, obj):\n",
    "        max_q, max_t = res['max_q'], res['max_t']\n",
    "        dists = quatAngularDiffBatch(max_q, to_np(grid_vertices))\n",
    "        bin_idx = np.argmin(dists)\n",
    "        lik_est = confusion_matrices[obj.item()][bin_idx]\n",
    "        lik_est /= lik_est.sum()\n",
    "        return lik_est, max_q, max_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_pose_utils.utils.interpolation import BinghamInterpolation, TetraInterpolation\n",
    "\n",
    "if(False):\n",
    "    mix_sigmas_data = np.load('/home/bokorn/src/DenseFusion/df_orig_sigmas.npz', allow_pickle=True)\n",
    "    sigmas_mixture = mix_sigmas_data['mixture_sigma'].item()\n",
    "    \n",
    "    sigmas_data = np.load('df_new_sigmas.npz', allow_pickle=True)\n",
    "    sigmas_single = sigmas_data['single_sigma'].item()\n",
    "    def bing_fixed(res, obj):\n",
    "        max_q, max_t = res['max_q'], res['max_t']\n",
    "        bingham = BinghamInterpolation(torch.Tensor(max_q).unsqueeze(0).cuda(), \n",
    "                                       torch.ones(1).cuda(), \n",
    "                                       sigma=torch.Tensor([sigmas_single[obj.item()]]).cuda())\n",
    "        return bingham, max_q, max_t\n",
    "    \n",
    "    def bing_mixed(res, obj):\n",
    "        max_q, max_t = res['max_q'], res['max_t']\n",
    "        pred_q, pred_t, pred_c = res['pred_q'], res['pred_t'], res['pred_c']\n",
    "\n",
    "        bingham_mixture = BinghamInterpolation(pred_q.cuda(), \n",
    "                                               pred_c.flatten().cuda(), \n",
    "                                               sigma=torch.Tensor(sigmas_mixture[obj.item()]).cuda())\n",
    "\n",
    "        return bingham_mixture, max_q, max_t\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<object_pose_utils.utils.interpolation.BinghamInterpolation at 0x7fecc4303390>"
      ]
     },
     "execution_count": 25,
       "dict_keys(['max_q', 'max_t', 'max_c', 'max_feat', 'pred_q', 'pred_t', 'pred_c', 'refine_q', 'refine_t', 'global_feat'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(False):\n",
    "    def hist_cosine(res, obj):\n",
    "        max_q, max_t, feat = res['max_q'], res['max_t'], res['global_feat']\n",
    "        lik_est = torch.mm(grid_features[obj.item()].cuda(), feat.transpose(0,1)).flatten()\n",
    "        lik_est /= grid_features[obj.item()].cuda().norm(dim=1)*feat.norm()\n",
    "        lik_est /= lik_est.sum()\n",
    "        return lik_est, max_q, max_t\n",
    "\n",
    "\n",
    "\n",
    "    def hist_uniform(res, obj):\n",
    "        max_q, max_t = res['max_q'], res['max_t']\n",
    "        lik_est = torch.ones(grid_size)\n",
    "        lik_est /= lik_est.sum()\n",
    "        return lik_est, max_q, max_t"
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution Estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from se3_distributions.losses.loglik_loss import evaluateFeature\n",
    "from se3_distributions.models.compare_networks import SigmoidCompareNet, SigmoidNet\n",
    "\n",
    "#feature_root = '/scratch/bokorn/results/dense_fusion_global_feat/'\n",
    "feature_root = '/scratch/bokorn/results/dense_fusion_local_orig_feat/'\n",
    "feature_key = 'feat_global'\n",
    "feature_size = 1024\n",
    "\n",
    "grid_vertices = torch.load(os.path.join(feature_root, 'grid',\n",
    "    '{}_vertices.pt'.format(dataset.classes[1])))\n",
    "\n",
    "grid_features = {}\n",
    "for object_id in object_list:\n",
    "    grid_features[object_id] = torch.load(os.path.join(feature_root, 'grid',\n",
    "        '{}_{}_features.pt'.format(feature_key, dataset.classes[object_id])))\n",
    "\n",
    "comp_model_checkpoint_pattern = '/scratch/bokorn/results/log_lik/df_global_orig_comp/lr_1e-5/2019-09-03_22-37-08/weights/checkpoint_*.pth'\n",
    "comp_model_checkpoint = sorted(glob.glob(comp_model_checkpoint_pattern,\n",
    "                                        recursive=True))[-1]\n",
    "comp_estimator = SigmoidCompareNet(feature_size, len(object_list))\n",
    "comp_estimator.load_state_dict(torch.load(comp_model_checkpoint))\n",
    "comp_estimator.cuda();\n",
    "comp_estimator.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_pose_utils.utils.bingham import iso_loss_calculation, duel_loss_calculation\n",
    "\n",
    "class isoLikelihood(object):\n",
    "        def __init__(self, mean_q, sig):\n",
    "            self.mean_q = mean_q\n",
    "            self.sig = sig.flatten()\n",
    "            \n",
    "        def __call__(self, quats):\n",
    "            likelihoods = []\n",
    "            for q in quats.unsqueeze(1):\n",
    "                loss, lik = iso_loss_calculation(self.mean_q, torch.abs(self.sig), q)\n",
    "                likelihoods.append(lik*2.0)\n",
    "            return torch.stack(likelihoods)\n",
    "        \n",
    "class duelLikelihood(object):\n",
    "        def __init__(self, mean_q, duel_q, z):\n",
    "            self.mean_q = mean_q\n",
    "            self.duel_q = duel_q\n",
    "            self.z = z.flatten()\n",
    "            \n",
    "        def __call__(self, quats):\n",
    "            likelihoods = []\n",
    "            for q in quats.unsqueeze(1):\n",
    "                loss, lik = duel_loss_calculation(self.mean_q, self.duel_q, \n",
    "                                                  -torch.abs(self.z), q)\n",
    "                likelihoods.append(lik*2.0)\n",
    "            return torch.stack(likelihoods)\n"
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_root = '/scratch/bokorn/results/dense_fusion_global_feat/'\n",
    "feature_key = 'feat_global'\n",
    "feature_size = 1024\n",
    "grid_size = 3885\n",
    "\n",
    "reg_model_checkpoint_pattern = '/scratch/bokorn/results/log_lik/df_global_orig_reg/lr_1e-5/**/weights/checkpoint_*.pth'\n",
    "reg_model_checkpoint = sorted(glob.glob(reg_model_checkpoint_pattern,\n",
    "                                        recursive=True))[-1]\n",
    "\n",
    "reg_estimator = SigmoidNet(feature_size, len(object_list)*grid_size)\n",
    "reg_estimator.load_state_dict(torch.load(reg_model_checkpoint))\n",
    "reg_estimator.cuda();\n",
    "reg_estimator.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/scratch/bokorn/results/log_lik/df_local_full_orig_iso/lr_1e-5/2019-09-08_23-09-05/weights/best_quat.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-48c7c429bd58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miso_model_checkpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/*'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0miso_estimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIsoBingham\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0miso_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miso_model_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0miso_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0miso_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/se3_distributions/bpy/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[1;32m    363\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/scratch/bokorn/results/log_lik/df_local_full_orig_iso/lr_1e-5/2019-09-08_23-09-05/weights/best_quat.pth'"
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-5d92d03b68d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mreg_estimator_idv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mobject_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobject_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mreg_model_idv_checkpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreg_model_idv_checkpoint_pattern\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mreg_estimator_idv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobject_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSigmoidNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mreg_estimator_idv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobject_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreg_model_idv_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from se3_distributions.models.bingham_networks import IsoBingham\n",
    "from se3_distributions.losses.bingham_loss import isoLikelihood\n",
    "\n",
    "if(True):\n",
    "    #feature_size = 1024\n",
    "    feature_size = 1408\n",
    "    #iso_model_checkpoint_pattern = '/scratch/bokorn/results/log_lik/df_local_full_orig_iso/**/weights/checkpoint_*.pth'\n",
    "    #iso_model_checkpoint = sorted(glob.glob(iso_model_checkpoint_pattern,\n",
    "    #                                        recursive=True))[-1]\n",
    "    iso_model_checkpoint = '/scratch/bokorn/results/log_lik/df_local_full_orig_iso/lr_1e-5/2019-09-08_23-09-05/weights/best_quat.pth'\n",
    "    print(glob.glob('/'.join(iso_model_checkpoint.split('/')[:-1]) + '/*', recursive=True))\n",
    "    iso_estimator = IsoBingham(feature_size, len(object_list))\n",
    "    iso_estimator.load_state_dict(torch.load(iso_model_checkpoint))\n",
    "    iso_estimator.eval()\n",
    "    iso_estimator.cuda()\n",
    "\n",
    "    def bing_iso(res, obj):\n",
    "        max_q, max_t, feat = res['max_q'], res['max_t'], res['max_feat']\n",
    "        \n",
    "        feat = torch.Tensor(feat).unsqueeze(0).cuda()\n",
    "        mean_est = torch.Tensor(max_q).unsqueeze(0).cuda()\n",
    "        df_obj = torch.LongTensor(obj-1).unsqueeze(0).cuda()\n",
    "        sig_est = iso_estimator(feat.unsqueeze(0).cuda(), df_obj)\n",
    "        lik_est = isoLikelihood(mean_q=mean_est[0], \n",
    "                                sig=sig_est[0,0])\n",
    "        \n",
    "        return lik_est, max_q, max_t"
    "feature_root = '/scratch/bokorn/results/dense_fusion_global_feat/'\n",
    "feature_key = 'feat_global'\n",
    "feature_size = 1024\n",
    "grid_size = 3885\n",
    "reg_model_idv_checkpoint_pattern = '/scratch/bokorn/results/log_lik/df_global_orig_reg/{}/lr_1e-5/**/weights/final_*.pth'\n",
    "\n",
    "reg_estimator_idv = {}\n",
    "for object_id in object_list:\n",
    "    reg_model_idv_checkpoint = sorted(glob.glob(reg_model_idv_checkpoint_pattern.format(object_id), recursive=True))[-1]\n",
    "    reg_estimator_idv[object_id] = SigmoidNet(feature_size, grid_size)\n",
    "    reg_estimator_idv[object_id].load_state_dict(torch.load(reg_model_idv_checkpoint))\n",
    "    reg_estimator_idv[object_id].cuda();\n",
    "    reg_estimator_idv[object_id].eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/scratch/bokorn/results/log_lik/df_local_full_orig_iso/lr_1e-5/2019-09-08_23-09-05/weights/checkpoint_348000.pth',\n",
       " '/scratch/bokorn/results/log_lik/df_local_full_orig_iso/lr_1e-5/2019-09-08_23-09-05/weights/best_quat.pth']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/scratch/bokorn/results/log_lik/df_local_full_orig_duel/lr_1e-5/2019-09-07_18-57-12/weights/checkpoint_20000.pth', '/scratch/bokorn/results/log_lik/df_local_full_orig_duel/lr_1e-5/2019-09-07_18-57-12/weights/best_quat.pth']\n"
     ]
    }
   ],
   "source": [
    "from se3_distributions.models.bingham_networks import DuelBingham\n",
    "from se3_distributions.losses.bingham_loss import duelLikelihood\n",
    "\n",
    "if(False):\n",
    "\n",
    "    \n",
    "    #feature_size = 1024\n",
    "    feature_size = 1408\n",
    "    #duel_model_checkpoint_pattern = '/scratch/bokorn/results/log_lik/df_local_full_orig_duel/**/weights/checkpoint_*.pth'\n",
    "    #duel_model_checkpoint = sorted(glob.glob(duel_model_checkpoint_pattern,\n",
    "    #                                         recursive=True))[-1]\n",
    "    duel_model_checkpoint = '/scratch/bokorn/results/log_lik/df_local_full_orig_duel/lr_1e-5/2019-09-07_18-57-12/weights/best_quat.pth'\n",
    "    print(glob.glob('/'.join(duel_model_checkpoint.split('/')[:-1]) + '/*', recursive=True))\n",
    "    duel_estimator = DuelBingham(feature_size, len(object_list))\n",
    "    duel_estimator.load_state_dict(torch.load(duel_model_checkpoint))\n",
    "    duel_estimator.eval()\n",
    "    duel_estimator.cuda()\n",
    "    \n",
    "    def bing_duel(res, obj):\n",
    "        max_q, max_t, feat = res['max_q'], res['max_t'], res['max_feat']\n",
    "\n",
    "        feat = torch.Tensor(feat).unsqueeze(0).cuda()\n",
    "        mean_est = torch.Tensor(max_q).unsqueeze(0).cuda()\n",
    "        df_obj = torch.LongTensor(obj-1).unsqueeze(0).cuda()\n",
    "        \n",
    "        duel_est, z_est = duel_estimator(feat, df_obj)\n",
    "\n",
    "        lik_est = duelLikelihood(mean_q=mean_est[0], \n",
    "                                 duel_q = duel_est[0,0],\n",
    "                                 z=z_est[0])\n",
    "        \n",
    "        return lik_est, max_q, max_t"
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_root = '/home/bokorn/data/confusion_matrices'\n",
    "confusion_eps = 0.000001\n",
    "\n",
    "confusion_matrices = {}\n",
    "for object_id in object_list:\n",
    "    confusion_matrices[object_id] = scio.loadmat(os.path.join(confusion_root, \n",
    "        \"{0}_confusion_matrix.mat\".format(object_id)))['loaded'] + confusion_eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/scratch/bokorn/results/log_lik/df_local_full_orig_duel/lr_1e-5/2019-09-07_18-57-12/weights/checkpoint_20000.pth',\n",
       " '/scratch/bokorn/results/log_lik/df_local_full_orig_duel/lr_1e-5/2019-09-07_18-57-12/weights/best_quat.pth']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([144.1236], device='cuda:0')\n",
      "True\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.6979, -0.3533,  0.2597,  0.5663])\n",
      "tensor([[[102.0641]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isoLoss(pred_mean, pred_sigma, true_r):\n",
    "    losses = []\n",
    "    likelihoods = []\n",
    "\n",
    "    for q, sig, x in zip(pred_mean, pred_sigma, true_r.unsqueeze(1)):\n",
    "        loss, lik = iso_loss_calculation(q, torch.abs(sig), x)\n",
    "        losses.append(loss)\n",
    "        likelihoods.append(lik)\n",
    "    \n",
    "    return torch.stack(likelihoods)\n",
    "\n",
    "def duelLoss(pred_mean ,pred_duel, pred_z, true_r):\n",
    "    losses = []\n",
    "    likelihoods = []\n",
    "\n",
    "    for qm, qd, z, x in zip(pred_mean, pred_duel, pred_z, true_r.unsqueeze(1)):\n",
    "        loss, lik = duel_loss_calculation(qm, qd, -torch.abs(z), x)\n",
    "        losses.append(loss)\n",
    "        likelihoods.append(lik)\n",
    "    \n",
    "    return torch.stack(likelihoods)\n",
    "        \n"
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_pose_utils.utils.bingham import isobingham_likelihood\n",
    "\n",
    "sigmas_data = np.load('/home/bokorn/src/DenseFusion/df_orig_sigmas.npz', allow_pickle=True)\n",
    "sigmas_single = sigmas_data['single_sigma'].item()\n",
    "sigmas_mixture = sigmas_data['mixture_sigma'].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[11.4916]], device='cuda:0')"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7198cebc66b5460da6acd4ad0cff1903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=14025), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_pose_utils.utils.pose_processing import quatAngularDiffBatch\n",
    "from object_pose_utils.utils.interpolation import BinghamInterpolation, TetraInterpolation\n",
    "from se3_distributions.utils.evaluation_utils import evaluateDenseFusion\n",
    "\n",
    "def hist_reg_global(res, obj):\n",
    "    max_q, max_t, feat = res['max_q'], res['max_t'], res['global_feat']\n",
    "    lik_est = evaluateFeature(reg_estimator, obj, feat, None)\n",
    "    lik_est = to_np(lik_est.flatten())\n",
    "    lik_est /= lik_est.sum()\n",
    "    return lik_est, max_q, max_t\n",
    "\n",
    "def hist_reg_idv_global(res, obj):\n",
    "    max_q, max_t, feat = res['max_q'], res['max_t'], res['global_feat']\n",
    "    lik_est = evaluateFeature(reg_estimator_idv[obj.item()], obj, feat, None)\n",
    "    lik_est = to_np(lik_est.flatten())\n",
    "    lik_est /= lik_est.sum()\n",
    "    return lik_est, max_q, max_t\n",
    "\n",
    "def hist_comp_global(res, obj):\n",
    "    max_q, max_t, feat = res['max_q'], res['max_t'], res['global_feat']\n",
    "    lik_est = evaluateFeature(comp_estimator, obj, feat, grid_features)\n",
    "    lik_est = to_np(lik_est.flatten())\n",
    "    lik_est /= lik_est.sum()\n",
    "    return lik_est, max_q, max_t\n",
    "\n",
    "def hist_cosign(res, obj):\n",
    "    max_q, max_t, feat = res['max_q'], res['max_t'], res['global_feat']\n",
    "    lik_est = torch.mm(grid_features[1].cuda(), feat.transpose(0,1)).flatten()\n",
    "    lik_est /= grid_features[1].cuda().norm(dim=1)*feat.norm()\n",
    "    lik_est /= lik_est.sum()\n",
    "    return lik_est, max_q, max_t\n",
    "\n",
    "def hist_conf(res, obj):\n",
    "    max_q, max_t = res['max_q'], res['max_t']\n",
    "    dists = quatAngularDiffBatch(max_q, to_np(grid_vertices))\n",
    "    bin_idx = np.argmin(dists)\n",
    "    lik_est = confusion_matrices[obj.item()][bin_idx]\n",
    "    lik_est /= lik_est.sum()\n",
    "    return lik_est, max_q, max_t\n",
    "\n",
    "def bing_fixed(res, obj):\n",
    "    max_q, max_t = res['max_q'], res['max_t']\n",
    "    bingham = BinghamInterpolation(torch.Tensor(max_q).unsqueeze(0).cuda(), \n",
    "                                   torch.ones(1).cuda(), \n",
    "                                   sigma=torch.Tensor(sigmas_single[obj.item()]).cuda())\n",
    "    return bingham, max_q, max_t\n",
    "    \n",
    "def bing_mixed(res, obj):\n",
    "    max_q, max_t = res['max_q'], res['max_t']\n",
    "    pred_q, pred_t, pred_c = res['pred_q'], res['pred_t'], res['pred_c']\n",
    "    \n",
    "    bingham_mixture = BinghamInterpolation(pred_q.cuda(), \n",
    "                                           pred_c.flatten().cuda(), \n",
    "                                           sigma=torch.Tensor(sigmas_mixture[obj.item()]).cuda())\n",
    "    \n",
    "    return bingham_mixture, max_q, max_t\n",
    "\n",
    "lik_funcs = {#'hist_reg_global':hist_reg_global,\n",
    "             #'hist_reg_idv_global':hist_reg_idv_global,\n",
    "             #'hist_comp_global':hist_comp_global,\n",
    "             #'hist_conf':hist_conf,\n",
    "             'hist_cosign':hist_cosign\n",
    "             #'bing_fixed':bing_fixed,\n",
    "             #'bing_mixed':bing_mixed,\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_results = df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bb1c081db704154a9b206d73847d0a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=14025), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "estimator_results = df_results\n",
    "lik_funcs = {#'hist_reg_local':hist_reg_local,\n",
    "             #'hist_reg_global':hist_reg_global,\n",
    "             #'hist_reg_indv_global_all':hist_reg_idv_global,\n",
    "             #'hist_comp_local':hist_comp_local,\n",
    "             #'hist_comp_global':hist_comp_global,\n",
    "             #'hist_conf_orig':hist_conf,\n",
    "             #'hist_cosine':hist_cosine\n",
    "             #'hist_uniform':hist_uniform\n",
    "             #'bing_fixed':bing_fixed,\n",
    "             #'bing_mixed':bing_mixed,\n",
    "             'bing_iso':bing_iso,\n",
    "             #'bing_duel':bing_duel,\n",
    "             }\n",
    "\n",
    "from object_pose_utils.utils.interpolation import TetraInterpolation, BinghamInterpolation\n",
    "from se3_distributions.losses.bingham_loss import isoLikelihood, duelLikelihood\n",
    "\n",
    "from se3_distributions.datasets.ycb_dataset import getYCBSymmeties\n",
    "from object_pose_utils.utils.pose_processing import symmetricAngularDistance, meanShift\n",
    "from object_pose_utils.utils.pose_error import add, adi\n",
    "from quat_math import quaternion_matrix\n",
    "\n",
    "tetra_interp = TetraInterpolation(2)\n",
    "\n",
    "import pathlib\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "likelihood = {k:{obj:{} for obj in object_list} for k in lik_funcs.keys()}\n",
    "\n",
    "sym_angular_error = {k:{obj:{} for obj in object_list} for k in lik_funcs.keys()}\n",
    "add_error = {k:{obj:{} for obj in object_list} for k in lik_funcs.keys()}\n",
    "add_sym_error = {k:{obj:{} for obj in object_list} for k in lik_funcs.keys()}\n",
    "\n",
    "sym_angular_error_mode = {k:{obj:{} for obj in object_list} for k in lik_funcs.keys()}\n",
    "add_error_mode = {k:{obj:{} for obj in object_list} for k in lik_funcs.keys()}\n",
    "add_sym_error_mode = {k:{obj:{} for obj in object_list} for k in lik_funcs.keys()}\n",
    "\n",
    "lik_distribution = {k:{obj:{} for obj in object_list} for k in lik_funcs.keys()}\n",
    "\n",
    "bad_data = []\n",
    "with torch.no_grad():\n",
    "    for j, data in enumerate(tqdm(dataset)):\n",
    "        obj, quat, trans, mat = data\n",
    "\n",
    "        if(len(obj) == 0):\n",
    "            bad_data.append(j)\n",
    "            continue\n",
    "\n",
    "        res = estimator_results[j]\n",
    "        if(res is None):\n",
    "            continue\n",
    "\n",
    "        sym_axis, sym_ang = getYCBSymmeties(obj.item())\n",
    "        trans = to_np(trans)\n",
    "\n",
    "        for k, func in lik_funcs.items():            \n",
    "            lik_est, q_est, t_est = func(res, obj)\n",
    "\n",
    "            if(type(lik_est) in [BinghamInterpolation, isoLikelihood, duelLikelihood]):\n",
    "                lik = lik_est(quat.unsqueeze(0).cuda()).item()\n",
    "                q_mode = q_est\n",
    "            else:\n",
    "                if(type(lik_est) is torch.Tensor):\n",
    "                    lik_est = to_np(lik_est.flatten())\n",
    "                tetra_interp.setValues(lik_est)\n",
    "                lik = tetra_interp.smooth(to_np(quat)).item()    \n",
    "                q_mode = grid_vertices[np.argmax(lik_est)]\n",
    "                #v_shift = meanShift(q_mode.cuda(), grid_vertices.cuda(), dist_est.cuda(),\n",
    "                #                    sigma=np.pi/9, max_iter = 100)\n",
    "\n",
    "            mat = quaternion_matrix(quat)\n",
    "\n",
    "            err_ang = symmetricAngularDistance(torch.Tensor(q_est).unsqueeze(0), \n",
    "                                               quat.unsqueeze(0),\n",
    "                                               sym_axis, sym_ang).item()*180/np.pi\n",
    "            mat_est = quaternion_matrix(q_est)\n",
    "            err_add = add(mat[:3,:3], trans, mat_est[:3,:3], t_est, \n",
    "                          model_clouds[obj.item()])\n",
    "            err_adi = adi(mat[:3,:3], trans, mat_est[:3,:3], t_est, \n",
    "                          model_clouds[obj.item()])\n",
    "\n",
    "            err_ang_mode = symmetricAngularDistance(torch.Tensor(q_mode).unsqueeze(0), \n",
    "                                                    quat.unsqueeze(0),\n",
    "                                                    sym_axis, sym_ang).item()*180/np.pi\n",
    "\n",
    "            mat_mode = quaternion_matrix(q_mode)\n",
    "            err_add_mode = add(mat[:3,:3], trans, mat_mode[:3,:3], t_est, \n",
    "                               model_clouds[obj.item()])\n",
    "            err_adi_mode = adi(mat[:3,:3], trans, mat_mode[:3,:3], t_est, \n",
    "                               model_clouds[obj.item()])\n",
    "\n",
    "            likelihood[k][obj.item()][j]=lik  \n",
    "            sym_angular_error[k][obj.item()][j]=err_ang\n",
    "            add_error[k][obj.item()][j]=err_add\n",
    "            add_sym_error[k][obj.item()][j]=err_adi\n",
    "\n",
    "            sym_angular_error_mode[k][obj.item()][j]=err_ang_mode\n",
    "            add_error_mode[k][obj.item()][j]=err_add_mode\n",
    "            add_sym_error_mode[k][obj.item()][j]=err_adi_mode\n",
    "\n",
    "            lik_distribution[k][obj.item()][j]=lik_est\n",
    "        \n",
    "np.savez('ral_results/final_single_img_{}_results.npz'.format('_'.join(list(lik_funcs.keys()))), \n",
    "\n",
    "for j, data in enumerate(tqdm(dataset)):\n",
    "    obj, quat, trans, mat = data\n",
    "    \n",
    "    if(len(obj) == 0):\n",
    "        bad_data.append(j)\n",
    "        continue\n",
    "    \n",
    "    res = estimator_results[j]\n",
    "    if(res is None):\n",
    "        continue\n",
    "    \n",
    "    sym_axis, sym_ang = getYCBSymmeties(obj.item())\n",
    "    trans = to_np(trans)\n",
    "        \n",
    "    for k, func in lik_funcs.items():            \n",
    "        lik_est, q_est, t_est = func(res, obj)\n",
    "        \n",
    "        if(type(lik_est) is BinghamInterpolation):\n",
    "            lik = lik_est(quat.unsqueeze(0).cuda()).item()\n",
    "            q_mode = q_est\n",
    "        else:\n",
    "            if(type(lik_est) is torch.Tensor):\n",
    "                lik_est = to_np(lik_est.flatten())\n",
    "            tetra_interp.setValues(lik_est)\n",
    "            lik = tetra_interp.smooth(to_np(quat)).item()    \n",
    "            q_mode = grid_vertices[np.argmax(lik_est)]\n",
    "            #v_shift = meanShift(q_mode.cuda(), grid_vertices.cuda(), dist_est.cuda(),\n",
    "            #                    sigma=np.pi/9, max_iter = 100)\n",
    "            \n",
    "        mat = quaternion_matrix(quat)\n",
    "\n",
    "        err_ang = symmetricAngularDistance(torch.Tensor(q_est).unsqueeze(0), \n",
    "                                           quat.unsqueeze(0),\n",
    "                                           sym_axis, sym_ang).item()*180/np.pi\n",
    "        mat_est = quaternion_matrix(q_est)\n",
    "        err_add = add(mat[:3,:3], trans, mat_est[:3,:3], t_est, \n",
    "                      model_clouds[obj.item()])\n",
    "        err_adi = adi(mat[:3,:3], trans, mat_est[:3,:3], t_est, \n",
    "                      model_clouds[obj.item()])\n",
    "        \n",
    "        err_ang_mode = symmetricAngularDistance(torch.Tensor(q_mode).unsqueeze(0), \n",
    "                                                quat.unsqueeze(0),\n",
    "                                                sym_axis, sym_ang).item()*180/np.pi\n",
    "        \n",
    "        mat_mode = quaternion_matrix(q_mode)\n",
    "        err_add_mode = add(mat[:3,:3], trans, mat_mode[:3,:3], t_est, \n",
    "                           model_clouds[obj.item()])\n",
    "        err_adi_mode = adi(mat[:3,:3], trans, mat_mode[:3,:3], t_est, \n",
    "                           model_clouds[obj.item()])\n",
    "\n",
    "        likelihood[k][obj.item()][j]=lik  \n",
    "        sym_angular_error[k][obj.item()][j]=err_ang\n",
    "        add_error[k][obj.item()][j]=err_add\n",
    "        add_sym_error[k][obj.item()][j]=err_adi\n",
    "        \n",
    "        sym_angular_error_mode[k][obj.item()][j]=err_ang_mode\n",
    "        add_error_mode[k][obj.item()][j]=err_add_mode\n",
    "        add_sym_error_mode[k][obj.item()][j]=err_adi_mode\n",
    "\n",
    "np.savez('ral_results/single_img_cosin_orig_results.npz', \n",
    "         likelihood=likelihood,\n",
    "         sym_angular_error=sym_angular_error,\n",
    "         add_error=add_error,\n",
    "         add_sym_error=add_sym_error,\n",
    "         sym_angular_error_mode=sym_angular_error_mode,\n",
    "         add_error_mode=add_error_mode,\n",
    "         add_sym_error_mode=add_sym_error_mode,\n",
    "        )\n",
    "np.savez('ral_results/final_single_img_{}_dists.npz'.format('_'.join(list(lik_funcs.keys()))), \n",
    "         lik_distribution=lik_distribution,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_q, max_t, feat = res['max_q'], res['max_t'], res['max_feat']\n",
    "feat = torch.Tensor(feat).unsqueeze(0)\n",
    "lik_est = evaluateFeature(local_comp_estimator, obj, feat, local_grid_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['max_q', 'max_t', 'max_c', 'max_feat', 'pred_q', 'pred_t', 'pred_c', 'refine_q', 'refine_t', 'global_feat'])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.keys()"
   ]
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected torch.FloatTensor (got torch.cuda.FloatTensor)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-0b5bfde14fa4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: expected torch.FloatTensor (got torch.cuda.FloatTensor)"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (bpy)",
   "language": "python",
   "name": "bpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
